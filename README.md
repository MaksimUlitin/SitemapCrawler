# SitemapCrawler


# Парсер SEO-данных из карты сайта (Sitemap)

Проект представляет собой инструмент для парсинга карты сайта (sitemap) и сбора базовых SEO-данных веб-страниц.

## Основные возможности
- Рекурсивный парсинг XML-карт сайта
- Извлечение ключевых SEO-параметров:
  - Title страницы
  - Заголовок H1
  - Мета-описание
  - HTTP статус код
- Поддержка случайных User-Agent для обхода блокировок
- Ограничение скорости запросов (throttling)
- Конкурентная обработка URL (goroutine-based)

## Установка
1. Клонировать репозиторий:
```bash
git clone https://github.com/maksimulitin/sitemap-seo-scraper.git
cd sitemap-seo-scraper
```

2. Установить зависимости:
```bash
go mod download
```

## Использование
Пример запуска (из `cmd/main.go`):
```go
p := model.DefaultParser{}
results := app.ScrapeSitemap("https://example.com/sitemap.xml", p, 10)
```

Параметры:
- URL карты сайта
- Реализация парсера (можно использовать DefaultParser или собственную)
- Уровень конкуренции (количество одновременных запросов)

## Структура проекта
```
.
├── cmd/               # Главный исполняемый модуль
│   └── main.go        # Точка входа
├── internal/          # Внутренние пакеты
│   ├── app/           # Основная логика приложения
│   └── util/          # Вспомогательные утилиты
├── pkg/               # Пакеты для повторного использования
│   └── model/         # Модели данных и интерфейсы
├── go.mod             # Модули Go
└── go.sum             # Зависимости
```

## Зависимости
- [goquery](https://github.com/PuerkitoBio/goquery) - парсинг HTML
- Стандартные библиотеки Go: net/http, sync, time

## Конфигурация
Для кастомизации можно:
1. Реализовать собственный парсер через интерфейс `model.Parser`
2. Добавить новые User-Agent в `pkg/model/userAg.go`
3. Настроить таймауты в `internal/app/scraper.go`
